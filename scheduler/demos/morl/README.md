# Minecart 环境

**Minecart 环境**是一个基于 Gymnasium 框架的强化学习环境，专注于多目标场景下的决策优化。代理需要在环境中平衡多个目标，完成任务并优化表现。

## 1. 环境目标

### 任务
控制小车在地图中移动，目标是：
- 收集两种矿物。
- 最小化燃料消耗。
- 返回基地以结束回合。

### 多目标奖励
奖励由三个部分组成：
1. 收集的第一种矿物数量（稀疏奖励）。
2. 收集的第二种矿物数量（稀疏奖励）。
3. 燃料消耗（负数，密集奖励）。

---

## 2. 核心组成

### 2.1 状态空间（Observation Space）
环境支持两种状态类型：
- **图像观测（`image_observation=True`）**：
  - 返回场景的 RGB 图像，形状为 `(WIDTH, HEIGHT, 3)`。
  - 数据类型为 `Box`，像素值范围为 `[0, 255]`。
- **向量观测（`image_observation=False`）**：
  - 一个 7 维向量：
    - \(x, y\)：小车的位置。
    - 速度：当前小车的速度。
    - \(\sin\) 和 \(\cos\)：小车的方向。
    - 小车内矿物占容量的百分比。

### 2.2 动作空间（Action Space）
动作空间为离散值，共 6 种动作：
- **0**: 挖掘矿物。
- **1**: 向左旋转。
- **2**: 向右旋转。
- **3**: 加速。
- **4**: 刹车。
- **5**: 无操作。

### 2.3 奖励空间（Reward Space）
奖励向量为三维：
1. 返回基地的第一种矿物数量。
2. 返回基地的第二种矿物数量。
3. 燃料消耗（负值）。

### 2.4 终止条件
当小车返回基地并提交矿物时，回合结束。

---

## 3. 核心功能

### 3.1 初始化
- **矿物生成**：
  - 矿物随机分布在地图上，避免与基地重叠。
  - 每个矿物有独立的分布（`scipy.stats.norm`），决定挖掘出的矿物数量。

### 3.2 状态更新
- **小车移动逻辑**：
  - 小车的位置通过速度和方向更新。
  - 撞墙时会调整方向（滑动）。
- **矿物挖掘逻辑**：
  - 挖掘的矿物量由矿物的分布决定。
  - 小车载荷不能超过容量。

### 3.3 奖励计算
- **燃料消耗**：
  - 每个动作都有燃料消耗，对应 `FUEL_LIST`。
- **矿物奖励**：
  - 小车返回基地时，根据矿物载荷计算奖励。

### 3.4 可视化
- **Pygame 渲染**：
  - 实时更新地图、基地、小车和矿物。
  - 提供两种模式：
    - `human`：显示在窗口中。
    - `rgb_array`：返回 RGB 图像。

---

## 4. 多目标优化支持

### Pareto 前沿
- 使用 `pareto_filter` 函数过滤非占优解，生成 Pareto 最优解集。
- 使用 `pareto_front` 方法计算近似的 Pareto 前沿。

### 凸覆盖集
- 使用 `convex_coverage_set` 方法结合 `ConvexHull` 计算凸覆盖集，处理目标间的非线性权衡。

---

## 5. 设计亮点

### 5.1 灵活的配置
- 环境参数可通过 JSON 文件配置，如矿物数量、载荷容量、燃料消耗等，便于适应不同任务需求。

### 5.2 动作帧跳跃
- 支持多帧跳跃（`frame_skip`），减少时间步长度，提高计算效率。

### 5.3 多模式渲染
- 提供 `human` 和 `rgb_array` 渲染模式，适用于可视化和训练场景。

---

## 6. 与强化学习的结合

### 与 Envelope Q-Learning 的配合

#### 多目标权重向量
- **Envelope Q-Learning** 使用权重向量 \(w\) 指导多目标优化。
- 训练过程中采样不同的 \(w\)，实现权重切换，覆盖目标空间。

#### 奖励向量化
- 环境直接返回奖励向量，便于算法计算包络目标（envelope target）或标量化奖励。

#### Pareto 前沿评估
- 推理阶段，算法可利用 `pareto_front` 方法评估策略在多目标下的表现。
